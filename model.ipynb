{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1a7658-4cc4-4540-9b7b-9d8a543a5446",
   "metadata": {},
   "source": [
    "Data Loading and Model Architecture Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3199a5-9a60-4028-8a6a-b4654abe4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "image_size = 256\n",
    "nz = 100  \n",
    "nc = 3    \n",
    "ngf = 64  \n",
    "ndf = 64  \n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "embedding_dim = 100  # Dimension of text embeddings\n",
    "\n",
    "# Create a dataset\n",
    "class FloorplanDataset(Dataset):\n",
    "    def __init__(self, image_dir, tags_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.tags_dir = tags_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(tags_dir, f.replace('png', 'txt')))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.filenames[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        tag_name = os.path.join(self.tags_dir, self.filenames[idx].replace('png', 'txt'))\n",
    "        with open(tag_name, 'r',encoding = \"utf8\") as file:\n",
    "            tags = file.read().replace('\\n', '')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        \n",
    "        tag_embedding = torch.zeros(embedding_dim)\n",
    "        words = tags.split()\n",
    "        for word in words:\n",
    "            index = abs(hash(word)) % embedding_dim\n",
    "            tag_embedding[index] = 1\n",
    "\n",
    "        return image, tag_embedding\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = FloorplanDataset(image_dir='Dataset/input/floorplan/floorplan_image', tags_dir='Dataset/input/tags/human_annotated_tags', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52a71b-881a-4179-acf1-9a6f88bc6a34",
   "metadata": {},
   "source": [
    "Conditional GAN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173bee33-f0a3-472e-9378-dbdc65737ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, embedding_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text = nn.Linear(embedding_dim, nz) \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(nz * 2, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, ngf // 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf // 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf // 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text_embedding):\n",
    "        \n",
    "        text_embedding = self.text(text_embedding)\n",
    "        text_embedding = text_embedding.view(noise.size(0), nz, 1, 1)  \n",
    "\n",
    "        combined_input = torch.cat([noise, text_embedding], 1)\n",
    "        return self.main(combined_input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, image_size, embedding_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.text = nn.Linear(embedding_dim, image_size * image_size)  \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 32, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, image, text_embedding):\n",
    "        \n",
    "        text_embedding = self.text(text_embedding).view(-1, 1, image_size, image_size)\n",
    "        \n",
    "        input = torch.cat([image, text_embedding], 1)\n",
    "        return self.main(input).view(-1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecda61b-f886-407e-8644-857097e46326",
   "metadata": {},
   "source": [
    "Evaluation Metrics and Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6db5978-e2df-4103-8139-01cacd811f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\LandTech\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\LandTech\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Initialize Inception model\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "def calculate_fid(real_imgs, fake_imgs, epsilon=1e-6):\n",
    "    # Ensure the Inception model is in eval mode and gradients are off\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get features from the Inception model, ensure they are detached from the graph and moved to CPU\n",
    "        real_activations = inception_model(real_imgs).detach().cpu().numpy()\n",
    "        fake_activations = inception_model(fake_imgs).detach().cpu().numpy()\n",
    "\n",
    "    # Calculate mean and covariance statistics for real and fake activations\n",
    "    mu1, sigma1 = real_activations.mean(axis=0), np.cov(real_activations, rowvar=False)\n",
    "    mu2, sigma2 = fake_activations.mean(axis=0), np.cov(fake_activations, rowvar=False)\n",
    "\n",
    "    # Adding a small epsilon to the diagonal of covariance matrices to ensure they are positive semi-definite\n",
    "    sigma1 += np.eye(sigma1.shape[0]) * epsilon\n",
    "    sigma2 += np.eye(sigma2.shape[0]) * epsilon\n",
    "\n",
    "    # Calculate the sum of squared differences between the means\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "\n",
    "    # Calculate sqrt of product of covariance matrices\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Check for complex numbers resulting from numerical inaccuracies and take the real part\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Calculate the Frechet Inception Distance\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "def calculate_inception_score(imgs, n_split=10):\n",
    "    # Calculate probabilities using inception model\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(inception_model(imgs), dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # Split predictions and calculate scores\n",
    "    scores = []\n",
    "    n_part = preds.shape[0] // n_split\n",
    "    for i in range(n_split):\n",
    "        part = preds[i * n_part:(i + 1) * n_part, :]\n",
    "        kl_div = part * (np.log(part) - np.log(np.expand_dims(part.mean(axis=0), 0)))\n",
    "        kl_div = kl_div.sum(axis=1)\n",
    "        scores.append(np.exp(kl_div.mean()))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca4232-335b-4450-993a-24163c3f0ce6",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abbf423-53e4-467f-90c4-57040b079cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and discriminator\n",
    "netG = Generator(nz, ngf, nc, embedding_dim).to(device)\n",
    "netD = Discriminator(ndf, nc, image_size, embedding_dim).to(device)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Noise for input to the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "real_acc_list = []\n",
    "fake_acc_list = []\n",
    "FID_scores = []\n",
    "IS_scores = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    real_correct = 0\n",
    "    fake_correct = 0\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        \n",
    "        # Apply label smoothing here for the real labels\n",
    "        real_labels = torch.full((batch_size,), 0.9, dtype=torch.float, device=device)  # Soft labels for real\n",
    "        fake_labels = torch.full((batch_size,), 0.1, dtype=torch.float, device=device)  # Soft labels for fake\n",
    "\n",
    "        output = netD(real_cpu, data[1].to(device))\n",
    "        errD_real = criterion(output, real_labels)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Calculate real accuracy\n",
    "        pred_real = output.detach() > 0.5  # True if discriminator thinks image is real\n",
    "        real_correct += pred_real.sum().item()\n",
    "        total_real += batch_size\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise, data[1].to(device))\n",
    "        output = netD(fake.detach(), data[1].to(device))\n",
    "        errD_fake = criterion(output, fake_labels)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # Calculate fake accuracy\n",
    "        pred_fake = output.detach() < 0.5  # True if discriminator thinks image is fake\n",
    "        fake_correct += pred_fake.sum().item()\n",
    "        total_fake += batch_size\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Update G network: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        output = netD(fake, data[1].to(device))\n",
    "        errG = criterion(output, real_labels)  # Fake labels are real for generator cost\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        \n",
    "    # Metric calculation every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Fetch a batch and separate images and embeddings\n",
    "            data = next(iter(dataloader))\n",
    "            real_images, embeddings = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Generate fake images for evaluation\n",
    "            fake_images = netG(fixed_noise, embeddings).to(device)  # Use the same embeddings for real and fake images\n",
    "\n",
    "            # Ensure the images are suitable for Inception V3\n",
    "            real_images_resized = F.interpolate(real_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            fake_images_resized = F.interpolate(fake_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Calculate FID and IS\n",
    "            fid = calculate_fid(real_images_resized, fake_images_resized)\n",
    "            inception_score = calculate_inception_score(fake_images_resized)\n",
    "\n",
    "            FID_scores.append(fid)\n",
    "            IS_scores.append(inception_score)\n",
    "            print(f'Epoch {epoch+1}: FID = {fid}, IS = {inception_score}')\n",
    "\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    # Calculate accuracies for the entire epoch\n",
    "    real_accuracy = real_correct / total_real\n",
    "    fake_accuracy = fake_correct / total_fake\n",
    "    real_acc_list.append(real_accuracy)\n",
    "    fake_acc_list.append(fake_accuracy)\n",
    "    print(f'Epoch {epoch + 1} Real Accuracy: {real_accuracy:.4f}, Fake Accuracy: {fake_accuracy:.4f}')\n",
    "\n",
    "    # Check how the generator is doing by saving G's output on fixed_noise\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        with torch.no_grad():\n",
    "            fake = netG(torch.randn(35, nz, 1, 1, device=device), torch.randn(batch_size, embedding_dim, device=device)).detach().to(device)\n",
    "        output_dir = '/Dataset/output'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_image(fake, f'/Dataset/output/fake_samples_epoch_{epoch}.png', normalize=True)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(netG.state_dict(), 'generator.pth')\n",
    "torch.save(netD.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42c36c-67a6-4f89-8396-64f50dde37b7",
   "metadata": {},
   "source": [
    "Evaluation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa97f78-f2f6-43d8-b1b5-33132aeb4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# epochs = list(range(1, num_epochs + 1))\n",
    "# metric_epochs = list(range(5, num_epochs + 1, 5))\n",
    "# iterations = list(range(1, len(D_losses) + 1))\n",
    "\n",
    "# # Plotting Discriminator and Generator Losses\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, D_losses, label='Discriminator Loss')\n",
    "# plt.plot(iterations, G_losses, label='Generator Loss')\n",
    "# plt.title('Training Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting Real and Fake Accuracy\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(epochs, real_acc_list, label='Real Accuracy')\n",
    "# plt.plot(epochs, fake_acc_list, label='Fake Accuracy')\n",
    "# plt.title('Discriminator Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting FID and IS Scores\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(metric_epochs, FID_scores, label='FID')\n",
    "# plt.title('Frechet Inception Distance')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('FID')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(metric_epochs, IS_scores, label='Inception Score')\n",
    "# plt.title('Inception Scores')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Score')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4ed6a-9e67-4216-b7c4-755f23579308",
   "metadata": {},
   "source": [
    "Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb30450-1082-4928-90fb-b9b2d7c7583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPAK\\AppData\\Local\\Temp\\ipykernel_16480\\319299788.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  netG.load_state_dict(torch.load('generator.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display  # For displaying images in Jupyter notebooks\n",
    "\n",
    "# Load pre-trained Generator\n",
    "netG = Generator(nz, ngf, nc, embedding_dim).to(device)\n",
    "netG.load_state_dict(torch.load('generator.pth', map_location=device))\n",
    "netG.eval()\n",
    "\n",
    "# Function to generate an image from text\n",
    "def generate_image_from_text(text):\n",
    "    # Convert text to embedding (simplified one-hot encoding as an example)\n",
    "    tag_embedding = torch.zeros(embedding_dim)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        index = abs(hash(word)) % embedding_dim\n",
    "        tag_embedding[index] = 1\n",
    "\n",
    "    # Generate noise vector\n",
    "    noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "    tag_embedding = tag_embedding.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Generate image\n",
    "    with torch.no_grad():\n",
    "        fake_image = netG(noise, tag_embedding)\n",
    "    \n",
    "    # Convert image tensor to PIL Image\n",
    "    fake_image = (fake_image * 0.5 + 0.5)  # unnormalize\n",
    "    img = transforms.ToPILImage()(fake_image.squeeze(0).cpu())\n",
    "    img.save(f'generated_image.png')  # Save the image\n",
    "\n",
    "    # Display the image\n",
    "    return img\n",
    "\n",
    "# generate_image_from_text(\"Balcony 1 is at the South corner position. between bathroom and kitchen. it is 8x8 sq ft Balcony 2 is at the North middle of the corner position. near living room. it is 14x3 sq ft. The bathroom is at the bathroom is center position in between common room2 and living room. it is 8x5 sq ft. Common room 1 is at the North middle of the corner position. between master room and living room. it is 10x10 sq ft. Common room 2 is at the West corner position. between bathroom and living room. it is 10x8 sq ft. The kitchen is at the South middle corner position. between living room and balcony. it is 10x5 sq ft. Living room is east center position it is south face. near all room located. it is  10x20 sq ft. The master room is at the West corner position. between common room1 and common room2. it is 10x10 sq ft.  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57b32c-9de0-49ff-ba71-5299f15042a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
